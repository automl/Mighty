# @package _global_
algorithm: DQN
q_func: ???

algorithm_kwargs:
  # Hyperparameters
  epsilon: 0.2  # Controls epsilon-greedy action selection in policy.
  td_update_class: mighty.mighty_update.DoubleQLearning

  replay_buffer_class:
    _target_: mighty.mighty_replay.PrioritizedReplay
  replay_buffer_kwargs:
    capacity: 1000000  # Maximum size of replay buffer.

  gamma: 0.9  # The amount by which to discount future rewards.

  # Training
  learning_rate: 0.001
  batch_size: 64  # Batch size for training.
#  begin_updating_weights: 1  # Begin updating policy weights after this many observed transitions.
  soft_update_weight: 0.01  # If we set :math:`\tau=1` we do a hard update. If we pick a smaller value, we do a smooth update.
  q_kwargs:
    dueling: False
    feature_extractor_kwargs:
      architecture: resnet
    head_kwargs:
      hidden_sizes: [512]