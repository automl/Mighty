# @package _global_
algorithm: BBF-e
q_func: ???

algorithm_kwargs:
  # Hyperparameters
  replay_ratio: 8
  population_size: 8
  configuration_points: 10
  shrinkage: 0.5
  offline_update_frac: 0.1

  base_agent_kwargs:
    replay_buffer_class: mighty.mighty_replay.PrioritizedReplay #Using prioritized experience replay
    replay_buffer_kwargs:
      capacity: 1000000  # Maximum size of replay buffer.
      alpha: 0.6

    # Training
    learning_rate: 0.001
    batch_size: 64  # Batch size for training.
    gamma: 0.9  # The amount by which to discount future rewards.
  #  begin_updating_weights: 1  # Begin updating policy weights after this many observed transitions.
    soft_update_weight: 1.  # If we set :math:`\tau=1` we do a hard update. If we pick a smaller value, we do a smooth update.
    td_update_class: mighty.mighty_update.QLearning #Simple Q-learning update instead of default DDQN
    q_kwargs:
      dueling: False
      feature_extractor_kwargs:
        architecture: mlp
        n_layers: 1
        hidden_sizes: [32]
      head_kwargs:
        hidden_sizes: [32]
