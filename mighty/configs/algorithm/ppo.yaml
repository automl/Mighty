# @package _global_
algorithm: PPO

algorithm_kwargs:
  # Hyperparameters
  n_policy_units: 8
  n_critic_units: 8
  soft_update_weight: 0.01

  rollout_buffer_class:
    _target_: mighty.mighty_replay.MightyRolloutBuffer  # Using rollout buffer
  rollout_buffer_kwargs:
    buffer_size: 2048  # Size of the rollout buffer.
    gamma: 0.99  # Discount factor for future rewards.
    gae_lambda: 0.95  # GAE lambda.
    obs_shape: ???  # Placeholder for observation shape
    act_dim: ???  # Placeholder for action dimension
    n_envs: ???

  # Training
  learning_rate: 0.001
  batch_size: 10000  # Batch size for training.
  gamma: 0.99  # The amount by which to discount future rewards.
  n_gradient_steps: 10  # Number of epochs for updating policy.
  ppo_clip: 0.2  # Clipping parameter for PPO.
  value_loss_coef: 0.5  # Coefficient for value loss.
  entropy_coef: 0.01  # Coefficient for entropy loss.
  max_grad_norm: 0.5  # Maximum value for gradient clipping.
  
  policy_class: mighty.mighty_exploration.StochasticPolicy  # Policy class for exploration
  policy_kwargs:
    entropy_coefficient: 0.2  # Coefficient for entropy-based exploration.
