# @package _global_
algorithm: DQN
q_func: ???

algorithm_kwargs:
  # Hyperparameters
  epsilon: 0.2  # Controls epsilon-greedy action selection in policy.

  replay_buffer_class:
    _target_: mighty.mighty_replay.MightyReplay
  replay_buffer_kwargs:
    capacity: 1000000  # Maximum size of replay buffer.
    random_seed: ${seed}

  tracer_class:
    _target_: coax.reward_tracing.NStep
  tracer_kwargs:
    n: 1  # The number of steps over which to bootstrap.
    gamma: 0.9  # The amount by which to discount future rewards.

  # Training
  learning_rate: 0.001
  batch_size: 64  # Batch size for training.
#  begin_updating_weights: 1  # Begin updating policy weights after this many observed transitions.
  soft_update_weight: 1.  # If we set :math:`\tau=1` we do a hard update. If we pick a smaller value, we do a smooth update.