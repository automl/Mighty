debug: false
seed: 0
output_dir: runs
wandb_project: null
tensorboard_file: null
experiment_name: mighty_experiment
algorithm_kwargs:
  n_units: 8
  epsilon: 0.2
  replay_buffer_class:
    _target_: mighty.mighty_replay.PrioritizedReplay
  replay_buffer_kwargs:
    capacity: 1000000
    alpha: 0.6
  learning_rate: 0.001
  batch_size: 64
  gamma: 0.9
  soft_update_weight: 1.0
  td_update_class: mighty.mighty_update.QLearning
  q_kwargs:
    dueling: false
    feature_extractor_kwargs:
      architecture: mlp
      n_layers: 1
      hidden_sizes:
      - 32
    head_kwargs:
      hidden_sizes:
      - 32
eval_every_n_steps: 10000.0
n_episodes_eval: 10
checkpoint: null
save_model_every_n_steps: 500000.0
algorithm: DQN
q_func: ???
num_steps: 200000.0
env: MiniGrid-LavaGapS5-v0
env_kwargs: {}
env_wrappers:
- minigrid.wrappers.ImgObsWrapper
- mighty.utils.wrappers.MinigridImgVecObs
num_envs: 1
search_space:
  hyperparameters:
    algorithm_kwargs.learning_rate:
      type: uniform_float
      lower: 1.0e-06
      upper: 0.01
      log: true
      default_value: 0.005
    algorithm_kwargs.epsilon:
      type: uniform_float
      lower: 0.01
      upper: 0.25
      default_value: 0.1
    algorithm_kwargs.batch_size:
      type: categorical
      choices:
      - 32
      - 64
      - 128
      - 256
      default_value: 32
    algorithm_kwargs.soft_update_weight:
      type: uniform_float
      lower: 0.01
      upper: 1.0
      log: true
      default_value: 1.0
    algorithm_kwargs.td_update_class:
      type: categorical
      choices:
      - mighty.mighty_update.QLearning
      - mighty.mighty_update.DoubleQLearning
      default_value: mighty.mighty_update.DoubleQLearning
